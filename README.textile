h1. RNASeq pipeline

h2. Setup

After cloning the repository, run the bootstrapping script to create the
required Python environment:

bc. $ ./bootstrap.sh

There's no harm in accidentally running this script multiple times.

Next, edit @config.sample.yaml@ and save it as @config.yaml@. Now you can try
whether the configuration is in a good state:

bc. $ ./check.py

Or get more detailed information with:

bc. $ ./check.py --detailed

h2. Documentation

AbstractStep:

* has 0 or 1 dependency to another step (if we need more than 1, we have to
  upgrade to a full DAG instead of just a tree)

run_info:

Each step consists of a couple of runs, the exact number of which is
determined by the step based on the input files. The @run_info@ structure
has the following format:

run id 1:
  output file 1:
    - input file 1
    - input file 2
...

For each run, a number of output files is generated, and every output file
depends on a number of input files. Different output files may depend on
the same input files. These dependencies are declared so that it's possible
to determine whether an output file is up-to-date based on the input files'
timestamps or whether steps have to be re-run.

@setup_runs@

The method @setup_runs@ must be implemented by every step. It receives a
@input_run_info@ dict which contains for every @run_id@ a list of output
file base names that will be generated by that step. The task of @setup_runs@
is to build a dict similar to the argument it received, but also including
for every output file the input files the output file depends on
(again, basenames only).

h3. Steps, runs, and tasks

Steps are processing steps like @cutadapt@, @fix_cutadapt@, etc. Every step
is implemented as a class which is derived from @AbstractStep@. A special
step class is @Source@ which provides the input files for all subsequent
steps, grouped by sample.

Every step class is able to define a number of runs based on its input files,
via the @setup_runs@ function. One run corresponds to one job executed
locally or on the cluster.

In the context of the pipeline, the sum of all runs of all steps is the set
of all tasks. The term 'task' exists only in the context of a pipeline and
describes a certain run of a certain step.

