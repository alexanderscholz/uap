h1. RNASeq pipeline

h2. Introduction

The aim of this data processing pipeline is to enable simple and robust bioinformatics data evaluation.

h3. Simplicity

* The entire processing pipeline is described as a tree, and output files are written into a directory structure mirroring this tree.
* To add a new processing step, a single Python file must be placed in @include/step@ which defines a class with two functions, one for planning all jobs based on a list of input files or runs and possibly additional information from previous steps and another function for running a specific job.

h3. Robustness

* All steps write their output files to a temporary location (a fact which a step is not aware of). Only if a step has completed successfully, the output files are copied to the correct output directory.
* The output directory names are suffixed with a eight-character hashtag which mirrors the options specified for the step.
* Processing can be aborted and continued from the command line at any time. This way, cluster failures are less critical.

h2. Setup

After cloning the repository, run the bootstrapping script to create the required Python environment (which will be located in @./python_env/@):

bc. $ ./bootstrap.sh

There's no harm in accidentally running this script multiple times.

h3. The configuration file

Next, edit @config.sample.yaml@ and save it as @config.yaml@. Here is a sample configuration:

bc. # This is the rnaseq-pipeline configuration file.
sources:
   - run_folder_source: { path: /home/michael/Desktop/rnaseq-pipeline/in }
destination_path: /home/michael/Desktop/rnaseq-pipeline/out
steps: |
    - cutadapt {
        adapter-R1: "AGATCGGAAGAGCACACGTCTGAACTCCAGTCAC((INDEX))ATCTCGTATGCCGTCTTCTGCTTG"
        adapter-R2: "AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGTAGATCTCGGTGGTCGCCGTATCATT"
      }
        - fix_cutadapt
tools:
    cutadapt:
        path: '/home/michael/Desktop/rnaseq-pipeline/tools/cutadapt-1.2.1/bin/cutadapt'
        get_version: '--version'
    pigz:
        path: 'pigz'
        get_version: '--version'
    dd:
        path: 'dd'
        get_version: '--version'

You can comment out lines with @#@. Steps can be defined as a tree. However, the syntax is a bit peculiar: The @|@ after @steps:@ defines a string spanning multiple lines in which line breaks and indentation is maintained. The string is later parsed by the pipeline and the most important parts are the individual steps which are to be performed. The relationship betweens steps is declared via indentation.

Steps may have options, which must be placed in between @{@ curly braces @}@ . Options can be specified on a single line (in this case, individual key/value pairs must be separated by comma) or may span multiple lines as shown in the example above, following standard YAML block syntax.

h4. The break step

There's a special step called @break@. It may be placed multiple times in the tree and it cuts off further processing from where it is. This may be helpful for building up a pipeline step by step.

h3. External tools

Tools are also specified in the config file, along with a command line parameter which is used by the pipeline to both test whether the tool is available and to record the version in log files.

h2. Scripts

Once the project is set up, there are several scripts which can be used to execute and monitor the pipeline. There are a couple of global command line parameters which are valid for all scripts:

<dl>
<dt>@--test-run@:</dt>
<dd>When this parameter is specified, a @head@ step is placed before all first-level steps in the step tree, which returns the first 1000 lines of every input file. That way, a pipeline can be tested very quickly with a small input data set. Also, output files will go to the output directory specified in the configuration plus @/test@.</dd>
</dl>

h3. status.py

The status script shows you whether the configuration is in a good state (if it doesn't complain, it's good) and which tasks are resulting from the configuration and the input files:

bc. $ ./status.py
[r] cutadapt/RIB0000784-R1
[r] cutadapt/RIB0000784-R2
[r] cutadapt/RIB0000794-R1
[r] cutadapt/RIB0000794-R2
[r] cutadapt/RIB0000770-R2
[r] cutadapt/RIB0000770-R1
[r] cutadapt/RIB0000757-R1
[r] cutadapt/RIB0000757-R2
[r] cutadapt/RIB0000777-R2
[r] cutadapt/RIB0000778-R2
[r] cutadapt/RIB0000778-R1
[r] cutadapt/RIB0000777-R1
[r] cutadapt/RIB0000740-R1
[r] cutadapt/RIB0000740-R2
[r] cutadapt/RIB0000786-R2
[r] cutadapt/RIB0000786-R1
[w] cutadapt/fix_cutadapt/RIB0000757
[w] cutadapt/fix_cutadapt/RIB0000770
[w] cutadapt/fix_cutadapt/RIB0000740
[w] cutadapt/fix_cutadapt/RIB0000777
[w] cutadapt/fix_cutadapt/RIB0000778
[w] cutadapt/fix_cutadapt/RIB0000794
[w] cutadapt/fix_cutadapt/RIB0000784
[w] cutadapt/fix_cutadapt/RIB0000786
tasks: 24 total, 16 ready, 8 waiting

The status script shows which tasks are ready to run, waiting because input files are still missing, or finished.

h3. run-locally.py

The @run-locally.py@ script runs all tasks sequentially on the local machine. Feel free to cancel this script at any time, it won't put your project in a confused state.

h3. run-on-cluster.py

The @run-on-cluster.py@ script determines which tasks still have to carried out and submits the jobs to a GridEngine cluster by calling @qsub@. Dependencies are defined via the @-hold_jid@ option, which means that jobs that depend on other jobs won't get scheduled until their dependencies have been satisfied.

h2. Documentation

AbstractStep:

* has 0 or 1 dependency to another step (if we need more than 1, we have to upgrade to a full DAG instead of just a tree)

run_info:

Each step consists of a couple of runs, the exact number of which is determined by the step based on the input files. The @run_info@ structure has the following format:

run id 1:
  output file 1:
    - input file 1
    - input file 2
...

For each run, a number of output files is generated, and every output file depends on a number of input files. Different output files may depend on the same input files. These dependencies are declared so that it's possible to determine whether an output file is up-to-date based on the input files' timestamps or whether steps have to be re-run.

@setup_runs@

The method @setup_runs@ must be implemented by every step. It receives a @input_run_info@ dict which contains for every @run_id@ a list of output file base names that will be generated by that step. The task of @setup_runs@ is to build a dict similar to the argument it received, but also including for every output file the input files the output file depends on (again, basenames only).

h3. Steps, runs, and tasks

Steps are processing steps like @cutadapt@, @fix_cutadapt@, etc. Every step is implemented as a class which is derived from @AbstractStep@. A special step class is @Source@ which provides the input files for all subsequent steps, grouped by sample.

Every step class is able to define a number of runs based on its input files, via the @setup_runs@ function. One run corresponds to one job executed locally or on the cluster.

In the context of the pipeline, the sum of all runs of all steps is the set of all tasks. The term 'task' exists only in the context of a pipeline and describes a certain run of a certain step.

