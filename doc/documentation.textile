<div class='title'>RNASeq pipeline documentation</div>

#{toc(1..3)}

h1. Introduction

The aim of this data processing pipeline is to enable simple and robust bioinformatics data evaluation.

*Simplicity:*

* The entire processing pipeline is described via a config file. Step are defined in a tree, and output files are written into a directory structure mirroring this tree.
* To add a new processing step, a single Python file must be placed in @include/step@ which defines a class with two functions, one for planning all jobs based on a list of input files or runs and possibly additional information from previous steps and another function for running a specific job.

*Robustness:*

* All steps write their output files to a temporary location (a fact which a step is not aware of). Only if a step has completed successfully, the output files are copied to the correct output directory.
* The output directory names are suffixed with a eight-character hashtag which mirrors the options specified for the step.
* Processing can be aborted and continued from the command line at any time. This way, cluster failures are less critical.
* Comprehensive annotations are written to the output directories, allowing for later investigation.
* Errors are caught as early as possible. Tools are checked for availability, the entire processing pipeline is calculated in advance.

A pipeline is defined by two aspects:

* the steps it carries out, with dependencies defined via a tree
* its input samples

The combination of _steps_ and _samples_ result in a list of _tasks_, which can be executed sequentially or can be submitted to a cluster.

h1. Setup

After cloning the repository, run the bootstrapping script to create the required Python environment (which will be located in @./python_env/@):

<listing>
$ ./bootstrap.sh
</listing>

There's no harm in accidentally running this script multiple times.

h2. The configuration file

Next, edit @config.sample.yaml@ and save it as @config.yaml@. Although writing the configuration may seem a bit complicated, it pays off later because further interaction with the pipeline is quite simple. Here is a sample configuration:

<listing>
# This is the rnaseq-pipeline configuration file.
sources:
   - run_folder_source: { path: /home/michael/Desktop/rnaseq-pipeline/in }
destination_path: /home/michael/Desktop/rnaseq-pipeline/out
steps: |
    - cutadapt {
        adapter-R1: "AGATCGGAAGAGCACACGTCTGAACTCCAGTCAC((INDEX))ATCTCGTATGCCGTCTTCTGCTTG"
        adapter-R2: "AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGTAGATCTCGGTGGTCGCCGTATCATT"
      }
        - fix_cutadapt
tools:
    cutadapt:
        path: '/home/michael/Desktop/rnaseq-pipeline/tools/cutadapt-1.2.1/bin/cutadapt'
        get_version: '--version'
    pigz:
        path: 'pigz'
        get_version: '--version'
    dd:
        path: 'dd'
        get_version: '--version'
</listing>

In the configuration, the following aspects of the pipeline are defined:

* @sources@ - there can be multiple sources of different types:
** run folders
** plain fastq.gz files with additional information
* @destination_path@ - this is where result files, annotations and temporary files are written to
* @steps@ - defines the processing step arranged in a tree
* @tools@ - defines all tools used in the pipeline and how to determine their versions (for later reference)

h2. Sources

In the following, the different types of sources are described in detail.

h3. Run folder source

Here's an example:

<listing>
- run_folder_source: { path: /home/michael/Desktop/rnaseq-pipeline/in }
</listing>

This source looks for fastq.gz files in @path/Unaligned/Project_*/Sample_*@ and pulls additional information from CSV sample sheets it finds. It also makes sure that index information for all samples is coherent and unambiguous.

h3. FASTQ source

Here's an example:

<listing>
- fastq_source:
    pattern: /data/original-fastq/&#42;.fastq.gz
    group: (Sample_COPD_\d+)_R[12].fastq.gz
    indices: copd-barcodes.csv
</listing>

Input files are collected as defined by @pattern@ and grouped into samples according to @group@, which is a regular expression. All groups defined in the regex @(  )@ are used to construct the sample name, here it is used to declare that both R1 and R2 files belong to the same sample. Indices are read from the CSV file defined by @indices@.

h2. Steps

Steps are defined as a tree. However, the syntax is a bit peculiar: The @|@ after @steps:@ defines a string spanning multiple lines in which line breaks and indentation is maintained (this is YAML syntax). The string is later parsed by the pipeline and the most important parts are the individual steps which are to be performed. The relationship betweens steps is declared via indentation.

<div class='background'>
Why do we need the | symbol in the steps definition? Neither the list nor the dictionary syntax allow for a concise definition of a step tree with options. Think of the step definition as a nested list with an option hash attached to every item.
</div>

Steps may have options, which must be placed in between @{@ curly braces @}@ . Options can be specified on a single line (in this case, individual key/value pairs must be separated by comma) or may span multiple lines, following standard YAML block syntax.

h3. break

There's a special step called @break@. It may be placed multiple times in the tree and it cuts off further processing from where it is. This may be helpful for building up a pipeline step by step.

h3. head

The @head@ step produces a shortened version of every input file. It is automatically prepended to the pipeline if @--test-run@ is specified on the command line.

Parameters:

* @n@ (optional) - number of lines (default: 1000)

h2. Tools

Tools are also specified in the config file, along with a command line parameter which is used by the pipeline to both test whether the tool is available and to record the version in log files.

h1. Scripts

Once the project is set up, there are several scripts which can be used to execute and monitor the pipeline. There are a couple of global command line parameters which are valid for all scripts:

<dl>
<dt>@--test-run@:</dt>
<dd>When this parameter is specified, a @head@ step is placed before all first-level steps in the step tree, which returns the first 1000 lines of every input file. That way, a pipeline can be tested very quickly with a small input data set. Also, output files will go to the output directory specified in the configuration plus @/test@.</dd>
</dl>

h2. status.py

The status script shows you whether the configuration is in a good state (if it doesn't complain, it's good) and which tasks are resulting from the configuration and the input files:

<listing>
$ ./status.py
[r] cutadapt/RIB0000784-R1
[r] cutadapt/RIB0000784-R2
[r] cutadapt/RIB0000794-R1
[r] cutadapt/RIB0000794-R2
[r] cutadapt/RIB0000770-R2
[r] cutadapt/RIB0000770-R1
[r] cutadapt/RIB0000757-R1
[r] cutadapt/RIB0000757-R2
[r] cutadapt/RIB0000777-R2
[r] cutadapt/RIB0000778-R2
[r] cutadapt/RIB0000778-R1
[r] cutadapt/RIB0000777-R1
[r] cutadapt/RIB0000740-R1
[r] cutadapt/RIB0000740-R2
[r] cutadapt/RIB0000786-R2
[r] cutadapt/RIB0000786-R1
[w] cutadapt/fix_cutadapt/RIB0000757
[w] cutadapt/fix_cutadapt/RIB0000770
[w] cutadapt/fix_cutadapt/RIB0000740
[w] cutadapt/fix_cutadapt/RIB0000777
[w] cutadapt/fix_cutadapt/RIB0000778
[w] cutadapt/fix_cutadapt/RIB0000794
[w] cutadapt/fix_cutadapt/RIB0000784
[w] cutadapt/fix_cutadapt/RIB0000786
tasks: 24 total, 16 ready, 8 waiting
</listing>

The status script shows which tasks are ready to run, waiting because input files are still missing, or finished.

h2. run-locally.py

The @run-locally.py@ script runs all tasks sequentially on the local machine. Feel free to cancel this script at any time, it won't put your project in a confused state.

h2. run-on-cluster.py

The @run-on-cluster.py@ script determines which tasks still have to carried out and submits the jobs to a GridEngine cluster by calling @qsub@. Dependencies are defined via the @-hold_jid@ option, which means that jobs that depend on other jobs won't get scheduled until their dependencies have been satisfied.

h1. Extending the pipeline

h1. Documentation

AbstractStep:

* has 0 or 1 dependency to another step (if we need more than 1, we have to upgrade to a full DAG instead of just a tree)

run_info:

Each step consists of a couple of runs, the exact number of which is determined by the step based on the input files. The @run_info@ structure has the following format:

<listing>
run id 1:
  output file 1:
    - input file 1
    - input file 2
...
</listing>

For each run, a number of output files is generated, and every output file depends on a number of input files. Different output files may depend on the same input files. These dependencies are declared so that it's possible to determine whether an output file is up-to-date based on the input files' timestamps or whether steps have to be re-run.

@setup_runs@

The method @setup_runs@ must be implemented by every step. It receives a @input_run_info@ dict which contains for every @run_id@ a list of output file base names that will be generated by that step. The task of @setup_runs@ is to build a dict similar to the argument it received, but also including for every output file the input files the output file depends on (again, basenames only).

h2. Steps, runs, and tasks

Steps are processing steps like @cutadapt@, @fix_cutadapt@, etc. Every step is implemented as a class which is derived from @AbstractStep@. A special step class is @Source@ which provides the input files for all subsequent steps, grouped by sample.

Every step class is able to define a number of runs based on its input files, via the @setup_runs@ function. One run corresponds to one job executed locally or on the cluster.

In the context of the pipeline, the sum of all runs of all steps is the set of all tasks. The term 'task' exists only in the context of a pipeline and describes a certain run of a certain step.
